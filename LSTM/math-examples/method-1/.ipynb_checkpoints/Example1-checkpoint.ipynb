{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 4ms/step - loss: 0.4849\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 880us/step - loss: 0.2605\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 913us/step - loss: 0.2201\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 979us/step - loss: 0.2311\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 950us/step - loss: 0.1910\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 929us/step - loss: 0.2373\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 985us/step - loss: 0.2157\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 927us/step - loss: 0.1744\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 916us/step - loss: 0.1738\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 945us/step - loss: 0.1511\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 921us/step - loss: 0.1538\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 908us/step - loss: 0.1442\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 946us/step - loss: 0.1279\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 924us/step - loss: 0.1144\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 906us/step - loss: 0.0996\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 921us/step - loss: 0.0791\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 939us/step - loss: 0.0604\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 932us/step - loss: 0.0705\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 937us/step - loss: 0.0844\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 909us/step - loss: 0.1239\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 909us/step - loss: 0.0862\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 922us/step - loss: 0.0712\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 925us/step - loss: 0.0517\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 937us/step - loss: 0.0416\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 907us/step - loss: 0.0383\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 931us/step - loss: 0.0303\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 901us/step - loss: 0.0544\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 912us/step - loss: 0.0435\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 927us/step - loss: 0.0547\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 909us/step - loss: 0.0538\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 925us/step - loss: 0.0288\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 986us/step - loss: 0.0175\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 964us/step - loss: 0.0283\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 933us/step - loss: 0.0282\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 941us/step - loss: 0.0300\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 986us/step - loss: 0.0207\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 984us/step - loss: 0.0154\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 925us/step - loss: 0.0099\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 935us/step - loss: 0.0188\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 922us/step - loss: 0.0261\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 983us/step - loss: 0.0276\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 940us/step - loss: 0.0227\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 937us/step - loss: 0.0143\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 936us/step - loss: 0.0126\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 929us/step - loss: 0.0182\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 957us/step - loss: 0.0111\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 921us/step - loss: 0.0186\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 943us/step - loss: 0.0249\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 952us/step - loss: 0.0295\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 0.0257\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 981us/step - loss: 0.0142\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 982us/step - loss: 0.0083\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 940us/step - loss: 0.0156\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 938us/step - loss: 0.0107\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 904us/step - loss: 0.0094\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 919us/step - loss: 0.0076\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 925us/step - loss: 0.0069\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 906us/step - loss: 0.0112\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 888us/step - loss: 0.0070\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 888us/step - loss: 0.0058\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 893us/step - loss: 0.0076\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 899us/step - loss: 0.0265\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 890us/step - loss: 0.0463\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 900us/step - loss: 0.1245\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 890us/step - loss: 0.1965\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 899us/step - loss: 0.1141\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 918us/step - loss: 0.0912\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 894us/step - loss: 0.0856\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 888us/step - loss: 0.0815\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 893us/step - loss: 0.0767\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 899us/step - loss: 0.0639\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 900us/step - loss: 0.0639\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 892us/step - loss: 0.0631\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 902us/step - loss: 0.0505\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 880us/step - loss: 0.0689\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 894us/step - loss: 0.0749\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 893us/step - loss: 0.0465\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 891us/step - loss: 0.0508\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 896us/step - loss: 0.0496\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 902us/step - loss: 0.0455\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 925us/step - loss: 0.0376\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 905us/step - loss: 0.0426\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 905us/step - loss: 0.0431\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 895us/step - loss: 0.0539\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 927us/step - loss: 0.0483\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 929us/step - loss: 0.0538\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 914us/step - loss: 0.0289\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 933us/step - loss: 0.0435\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 934us/step - loss: 0.0365\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 885us/step - loss: 0.0344\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 907us/step - loss: 0.0266\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 899us/step - loss: 0.0162\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 903us/step - loss: 0.0188\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 906us/step - loss: 0.0109\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 930us/step - loss: 0.0077\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 0.0078\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 929us/step - loss: 0.0057\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 897us/step - loss: 0.0051\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 914us/step - loss: 0.0064\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 906us/step - loss: 0.0041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t+1 RMSE: 0.002075\n",
      "t+2 RMSE: 0.004096\n",
      "t+3 RMSE: 0.007480\n",
      "t+4 RMSE: 0.010327\n",
      "t+5 RMSE: 0.012808\n",
      "t+6 RMSE: 0.016478\n",
      "t+7 RMSE: 0.020306\n",
      "t+8 RMSE: 0.024047\n",
      "t+9 RMSE: 0.028484\n",
      "t+10 RMSE: 0.033383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "    print(x)\n",
    "    y = pd.Timestamp(1901, 1, 1, 12)\n",
    "    print(y)\n",
    "    return datetime.strptime('190'+x, '%Y-%m')\n",
    "\n",
    "# convert time series into supervised learning problem\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    "\n",
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "\t# extract raw values\n",
    "\traw_values = series.values\n",
    "\t# transform data to be stationary\n",
    "\tdiff_series = difference(raw_values, 1)\n",
    "\tdiff_values = diff_series.values\n",
    "\tdiff_values = diff_values.reshape(len(diff_values), 1)\n",
    "\t# rescale values to -1, 1\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaled_values = scaler.fit_transform(diff_values)\n",
    "\tscaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "\t# transform into supervised learning problem X, y\n",
    "\tsupervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "\tsupervised_values = supervised.values\n",
    "\t# split into train and test sets\n",
    "\ttrain, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "\treturn scaler, train, test\n",
    "\n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons):\n",
    "\t# reshape training into [samples, timesteps, features]\n",
    "\tX, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "\tX = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\t# design network\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "\tmodel.add(Dense(y.shape[1]))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\t# fit network\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=n_batch, verbose=1, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\treturn model\n",
    "\n",
    "# make one forecast with an LSTM,\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "\t# reshape input pattern to [samples, timesteps, features]\n",
    "\tX = X.reshape(1, 1, len(X))\n",
    "\t# make forecast\n",
    "\tforecast = model.predict(X, batch_size=n_batch)\n",
    "\t# convert to array\n",
    "\treturn [x for x in forecast[0, :]]\n",
    "\n",
    "# evaluate the persistence model\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n",
    "\tforecasts = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\tX, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "\t\t# make forecast\n",
    "\t\tforecast = forecast_lstm(model, X, n_batch)\n",
    "\t\t# store the forecast\n",
    "\t\tforecasts.append(forecast)\n",
    "\treturn forecasts\n",
    "\n",
    "# invert differenced forecast\n",
    "def inverse_difference(last_ob, forecast):\n",
    "\t# invert first forecast\n",
    "\tinverted = list()\n",
    "\tinverted.append(forecast[0] + last_ob)\n",
    "\t# propagate difference forecast using inverted first value\n",
    "\tfor i in range(1, len(forecast)):\n",
    "\t\tinverted.append(forecast[i] + inverted[i-1])\n",
    "\treturn inverted\n",
    "\n",
    "# inverse data transform on forecasts\n",
    "def inverse_transform(series, forecasts, scaler, n_test):\n",
    "\tinverted = list()\n",
    "\tfor i in range(len(forecasts)):\n",
    "\t\t# create array from forecast\n",
    "\t\tforecast = array(forecasts[i])\n",
    "\t\tforecast = forecast.reshape(1, len(forecast))\n",
    "\t\t# invert scaling\n",
    "\t\tinv_scale = scaler.inverse_transform(forecast)\n",
    "\t\tinv_scale = inv_scale[0, :]\n",
    "\t\t# invert differencing\n",
    "\t\tindex = len(series) - n_test + i - 1\n",
    "\t\tlast_ob = series.values[index]\n",
    "\t\tinv_diff = inverse_difference(last_ob, inv_scale)\n",
    "\t\t# store\n",
    "\t\tinverted.append(inv_diff)\n",
    "\treturn inverted\n",
    "\n",
    "# evaluate the RMSE for each forecast time step\n",
    "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n",
    "\tfor i in range(n_seq):\n",
    "\t\tactual = [row[i] for row in test]\n",
    "\t\tpredicted = [forecast[i] for forecast in forecasts]\n",
    "\t\trmse = sqrt(mean_squared_error(actual, predicted))\n",
    "\t\tprint('t+%d RMSE: %f' % ((i+1), rmse))\n",
    "\n",
    "# plot the forecasts in the context of the original dataset\n",
    "def plot_forecasts(series, forecasts, n_test):\n",
    "\t# plot the entire dataset in blue\n",
    "\tpyplot.plot(series.values)\n",
    "\t# plot the forecasts in red\n",
    "\tfor i in range(len(forecasts)):\n",
    "\t\toff_s = len(series) - n_test + i - 1\n",
    "\t\toff_e = off_s + len(forecasts[i]) + 1\n",
    "\t\txaxis = [x for x in range(off_s, off_e)]\n",
    "\t\tyaxis = [series.values[off_s]] + forecasts[i]\n",
    "\t\t#pyplot.plot(xaxis, yaxis, color='red')\n",
    "\t# show the plot\n",
    "\tpyplot.show()\n",
    "\n",
    "# load dataset\n",
    "#series = read_csv('data/shampoo-sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "#series = read_csv('data/shampoo-sales.csv', header=0, index_col=0, squeeze=True)\n",
    "#series = read_csv('data/data-train4.csv', header=0, index_col=0, squeeze=True)\n",
    "series  = read_csv('data/data-train-tanh-nonoise.csv', header=0, index_col=0, squeeze=True)\n",
    "\n",
    "# configure\n",
    "n_lag = 1\n",
    "n_seq = 10\n",
    "n_test = 10\n",
    "n_epochs = 100 #1500\n",
    "n_batch = 1\n",
    "n_neurons = 30\n",
    "# prepare data\n",
    "scaler, train, test = prepare_data(series, n_test, n_lag, n_seq)\n",
    "\n",
    "\n",
    "# fit model\n",
    "model = fit_lstm(train, n_lag, n_seq, n_batch, n_epochs, n_neurons)\n",
    "# make forecasts\n",
    "forecasts = make_forecasts(model, n_batch, train, test, n_lag, n_seq)\n",
    "# inverse transform forecasts and test\n",
    "forecasts = inverse_transform(series, forecasts, scaler, n_test+2)\n",
    "actual = [row[n_lag:] for row in test]\n",
    "actual = inverse_transform(series, actual, scaler, n_test+2)\n",
    "# evaluate forecasts\n",
    "evaluate_forecasts(actual, forecasts, n_lag, n_seq)\n",
    "# plot forecasts\n",
    "plot_forecasts(series, forecasts, n_test+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the forecasts in the context of the original dataset\n",
    "def plot_forecasts2(series, forecasts, n_test):\n",
    "\t# plot the entire dataset in blue\n",
    "\tpyplot.plot(series.values)\n",
    "\t# plot the forecasts in red\n",
    "\tfor i in np.arange(0,len(forecasts),10): #range(len(forecasts)):\n",
    "\t\toff_s = len(series) - n_test + i - 1\n",
    "\t\toff_e = off_s + len(forecasts[i]) + 1\n",
    "\t\txaxis = [x for x in range(off_s, off_e)]\n",
    "\t\tyaxis = [series.values[off_s]] + forecasts[i]\n",
    "\t\tpyplot.plot(xaxis, yaxis, color='red')\n",
    "\t# show the plot\n",
    "\tpyplot.grid(True)\n",
    "\tpyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-558301f62172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#evaluate_forecasts(actual2, forecasts2, n_lag, n_seq)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# plot forecasts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mplot_forecasts2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforecasts2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_test2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-d9b6f6e121a8>\u001b[0m in \u001b[0;36mplot_forecasts2\u001b[0;34m(series, forecasts, n_test)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# plot the forecasts in red\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecasts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#range(len(forecasts)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                 \u001b[0moff_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_test\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0moff_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moff_s\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecasts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8XXWd//HXJ3sXuu9rUlroBt1iAVFkKVAQWhAGYXAsCvY3P+WnMzKMMPjTEUcHxvkNs4hLRaAKUqCIjVCmsiqjtDSF7gtN0y1J26TpkrRp1vv5/XFP8SYk3e5Nzk3u+/l43Mc953u+5+bT0yTvnPVr7o6IiMhxaWEXICIiyUXBICIizSgYRESkGQWDiIg0o2AQEZFmFAwiItKMgkFERJpRMIiISDMKBhERaSYj7ALOxIABAzw3NzfsMkREOpVVq1btd/eBJ+vXKYMhNzeXwsLCsMsQEelUzGznqfTToSQREWlGwSAiIs0oGEREpBkFg4iINKNgEBGRZhISDGb2uJmVm9n6Npabmf2nmRWZ2Vozmx6zbJ6ZbQ1e8xJRj4iInLlE7TE8Ccw+wfJrgHHBaz7wYwAz6wd8G7gAmAl828z6JqgmERE5Awm5j8Hd/2BmuSfoMhf4hUfHEV1uZn3MbChwKfCqux8AMLNXiQbMM4moS0RSWyTi1DVGqG+K0NgUoTHiNDRFaGxyGiMRGpqcxianIRK0NUVoiATvTU7EHXeI+J+nHScSofn8h30Abz7vzT4j2t+DdqLdo+/Npv+87MPBl4OF8z6eS/+e2e263TrqBrfhwO6Y+ZKgra32jzCz+UT3Nhg1alT7VCkiSaGhKULlkXoOHavncE0Dh4999FV1rIGa+iaONTRR2xB9P1bfRG1DhJr6xqA9EvY/JaHMYM7UYV0mGOLm7guABQD5+fl+ku4ikqTcncqj9ezYf5Tt+4+y++Axyqtq2VdVy76qOsqra6k8Wv/hX88tmUHvbpn0ysmke1Y63bLS6ZaZTq+cTHKC6W6Z0faczHRyMtPISk8jMz2NjHQjMy36npGeRmZa9D22PTPdyAim09OMNDPSDMwMg2DeMIvWcnw+zYAW84ZhaTSfD9Y7Pk10teDfZjHTf27raB0VDKXAyJj5EUFbKdHDSbHtb3VQTSLSjtyd8uo6NpQdZkNpFVv2VbOj8ig799dQXdf4Yb80g/49sxncK5uhvXOYMrIPg3tlM/CsbPp2z6J3t8wPX726ZXJWdgZpaR3/yzKVdFQwFAB3m9kioieaD7v7HjNbBnw/5oTzVcD9HVSTiCRQQ1OEDWVVrNx+gBXbD7B690H2H6n/cPmoft3JG9CDGaP6kjugR/TVvwcj+nYjM11XzieThASDmT1D9C//AWZWQvRKo0wAd/8JsBS4FigCaoAvBMsOmNl3gZXBRz14/ES0iCS/8qpaXt9czmsb9/FOcSU19U0A5PbvzqfOGcTk4b2YNKw3E4aexVk5mSFXK6fKvK0DeUksPz/f9XRVkXCUV9Xy6/dLeWXdHtaUHAZgRN9uXHbuIC4Y04+Zuf0Y1Csn5CqlNWa2yt3zT9av05x8FpHw1DdGeGPzPp4rLOH3H1TQFHGmjOjNvVefy6wJgzlncM9QTpJK+1AwiEibjtY18qsVu3jsf4rZV1XH4F7ZzL9kDDfPGMHZA3uGXZ60EwWDiHzEoZp6nvjjDha+s4NDNQ1cNKY///yZ87hk3EAydKK4y1MwiMiHGpsiPLV8J//26gdU1TYya8JgvnzZ2UwfpSfVpBIFg4gAsLy4kn8s2MDmvdV8YuwAvnndBMYP6RV2WRICBYNIijtUU8+3CzawZHUZw/t04yefm87Vk4boZHIKUzCIpLDff1DBvc+v4cDRer56+Vj+96Vj6ZaVHnZZEjIFg0gKaoo4j7z6AT98s4hxg3ry+B0fY/Lw3mGXJUlCwSCSYg7XNPCVX73H/xTt57P5I/nO3EnkZGovQf5MwSCSQnbsP8oXF65k94Ea/uWm87nlYyNPvpKkHAWDSIpYV3KYeU+8i7vz9F0XMjOvX9glSZJSMIikgBXFldy5sJDe3TJ5+q4LyB3QI+ySJIkpGES6uOXFldzxxLsM79ONp+66gKG9u4VdkiQ5BYNIF7Zq5wG++ORKRvTtzqL5FzKgnYeElK5BDz0R6aK27K3mjidWMuisbH511wUKBTllCQkGM5ttZlvMrMjM7mtl+SNmtjp4fWBmh2KWNcUsK0hEPSKpbu/hWu544l26Zabz9Jcu1PgIclriPpRkZunAo8CVQAmw0swK3H3j8T7u/rcx/f8PMC3mI465+9R46xCRqKN1jdzxxLtUHWvgub++iOF9dE5BTk8i9hhmAkXuXuzu9cAiYO4J+t8GPJOArysiLbg79y5ewwf7qvnR52YwaZjuZpbTl4hgGA7sjpkvCdo+wsxGA3nAGzHNOWZWaGbLzeyGBNQjkrJ+9NY2lq7by33XjOdT5wwMuxzppDr6qqRbgcXu3hTTNtrdS81sDPCGma1z920tVzSz+cB8gFGjRnVMtSKdyJ+27edff7eF66cM40ufHBN2OdKJJWKPoRSIva9+RNDWmltpcRjJ3UuD92LgLZqff4jtt8Dd8909f+BA/SUkEqvySB1/s2g1eQN68PBN5+mR2RKXRATDSmCcmeWZWRbRX/4fubrIzMYDfYF3Ytr6mll2MD0AuBjY2HJdEWmbu/N3z6/h0LEG/uu2aXTP0u1JEp+4v4PcvdHM7gaWAenA4+6+wcweBArd/XhI3AoscnePWX0C8FMzixANqYdir2YSkZP71bu7eHNLBd+ZM0knmyUhrPnv6c4hPz/fCwsLwy5DJHS7D9Rw9b//gemj+vLLO2fqEJKckJmtcvf8k/XTnc8inVQk4vz94rWkmfHwzecrFCRhFAwindTzq3bzTnEl3/z0BN3EJgmlYBDphPYfqeP7SzczM68fn9VgO5JgCgaRTuj7L2+ipr6R7984WYeQJOEUDCKdzLvbD/Dr90uZf8kYxg46K+xypAtSMIh0IpGI8+BLGxjaO4e7LxsXdjnSRSkYRDqRxe+VsL60ivuuGU+3rPSwy5EuSsEg0kkcqWvkB8u2MH1UH+ZMGRZ2OdKFKRhEOonH3i6morqO/3vdRJ1wlnalYBDpBCqP1PGzPxRzzeQhTBvVN+xypItTMIh0Ao++uY1jDU3cc9W5YZciKUDBIJLkSg8d46nlO7klfyRjB/UMuxxJAQoGkST36JtFOM5Xr9DlqdIxFAwiSaz00DGeL9zNLfkjGabnIUkHUTCIJLEfv1UEwJcvGxtyJZJKFAwiSWrv4VqeW1nCzTNG6ump0qESEgxmNtvMtphZkZnd18ryO8yswsxWB6+7YpbNM7OtwWteIuoR6Qqe+ON2mtz58qVnh12KpJi4h/Y0s3TgUeBKoARYaWYFrQzR+ay7391i3X7At4F8wIFVwboH461LpDOrrm3gVyt2cc3kIYzs1z3sciTFJGKPYSZQ5O7F7l4PLALmnuK6VwOvuvuBIAxeBWYnoCaRTu3Zlbuprmtk/iVjwi5FUlAigmE4sDtmviRoa+kmM1trZovN7PjIIqe6rkjKaGiK8Pj/bOeCvH6cP6JP2OVICuqok8+/BXLd/XyiewULT/cDzGy+mRWaWWFFRUXCCxRJFkvX7aHscK32FiQ0iQiGUiB2bMERQduH3L3S3euC2ceAGae6bsxnLHD3fHfPHzhwYALKFkk+7s7P3i5mzMAeXHbuoLDLkRSViGBYCYwzszwzywJuBQpiO5jZ0JjZOcCmYHoZcJWZ9TWzvsBVQZtISnqnuJL1pVV86ZNjSEvTE1QlHHFfleTujWZ2N9Ff6OnA4+6+wcweBArdvQD4qpnNARqBA8AdwboHzOy7RMMF4EF3PxBvTSKd1WNvb2dAzyxunKZTbRKeuIMBwN2XAktbtH0rZvp+4P421n0ceDwRdYh0ZkXl1byxuZyvX3kOOZkanU3CozufRZLEU8t3kZWexu0XjAq7FElxCgaRJFBT38gL75Vw7XlD6N8zO+xyJMUpGESSwEtr9lBd28jtF44OuxQRBYNIMnh6xU7OGdyT/NEatlPCp2AQCdm6ksOsKTnM7ReMxkyXqEr4FAwiIXt6xU66ZaZz43RdoirJQcEgEqKq2gaWrC5jzpRh9MrJDLscEUDBIBKq37xfyrGGJm6/UJeoSvJQMIiExN15evkuzhveW09RlaSiYBAJyaqdB9myr1o3tEnSUTCIhOT5whK6Z6Vz/ZRhYZci0oyCQSQENfWNvLxuD58+byg9shPyyDKRhFEwiIRg2Ya9HKlr5KYZI8IuReQjFAwiIXhhVSkj+3VjZm6/sEsR+QgFg0gHKz10jD9u289N00doMB5JSgoGkQ724nsluMNN03UYSZJTQoLBzGab2RYzKzKz+1pZ/nUz22hma83sdTMbHbOsycxWB6+CluuKdCXuzgvvlXJBXj9G9usedjkirYo7GMwsHXgUuAaYCNxmZhNbdHsfyHf384HFwL/ELDvm7lOD15x46xFJZu/tOsj2/Ue5WSedJYklYo9hJlDk7sXuXg8sAubGdnD3N929JphdDuinQlLS4lXRexeuPW9o2KWItCkRwTAc2B0zXxK0teVO4JWY+RwzKzSz5WZ2Q1srmdn8oF9hRUVFfBWLhOBYfRMvrdnDNZN174Iktw797jSzzwH5wKdimke7e6mZjQHeMLN17r6t5bruvgBYAJCfn+8dUrBIAv1u416q6xq5aYYery3JLRF7DKXAyJj5EUFbM2Y2C3gAmOPudcfb3b00eC8G3gKmJaAmkaSzZHUZQ3vncGFe/7BLETmhRATDSmCcmeWZWRZwK9Ds6iIzmwb8lGgolMe09zWz7GB6AHAxsDEBNYkklYNH6/nDBxXMmTJM9y5I0ov7UJK7N5rZ3cAyIB143N03mNmDQKG7FwA/AHoCzwdDF+4KrkCaAPzUzCJEQ+ohd1cwSJezdP0eGiPOnKl6YJ4kv4ScY3D3pcDSFm3fipme1cZ6fwLOS0QNIslsyeoyxg7qycShvcIuReSkdOezSDsrO3SMd7cfYO6UYQR7zCJJTcEg0s5eWlsGoHEXpNNQMIi0syWry5gysg+5A3qEXYrIKVEwiLSjovJqNpRVMVd7C9KJKBhE2lHB6jLSDK47X4/AkM5DwSDSTtydgjVlfPzsAQzqlRN2OSKnTMEg0k7WlhxmR2WN7l2QTkfBINJOlqwuIys9jasnDQm7FJHTomAQaQdNEee3a8u4bPxAenfLDLsckdOiYBBpByuKK6mormPuVD1JVTofBYNIO1iyuoye2RlcPn5Q2KWInDYFg0iC1TU2sXT9Hq6eNISczPSwyxE5bQoGkQR7a0sF1bWNuhpJOi0Fg0iCFawpo3+PLC4+WwPySOekYBBJoKN1jby+aR/XnjeUjHT9eEnnlJDvXDObbWZbzKzIzO5rZXm2mT0bLF9hZrkxy+4P2reY2dWJqEckLK9t2kdtQ0SHkaRTizsYzCwdeBS4BpgI3GZmE1t0uxM46O5jgUeAh4N1JxIdCnQSMBv4UfB5Ip3Sb9dEx3WeMapv2KWInLFE7DHMBIrcvdjd64FFwNwWfeYCC4PpxcAVFh2xZC6wyN3r3H07UBR8nkinc7imgd9/UMF15w/VuM7SqSUiGIYDu2PmS4K2Vvu4eyNwGOh/iuuKdArLNuylocmZM0XfwtK5dZqzY2Y238wKzaywoqIi7HJEPqJgTRm5/bszebjGdZbOLRHBUAqMjJkfEbS12sfMMoDeQOUprguAuy9w93x3zx84cGACyhZJnIrqOv60bT/Xa1xn6QISEQwrgXFmlmdmWURPJhe06FMAzAumbwbecHcP2m8NrlrKA8YB7yagJpEO9cr6PURc4zpL15AR7we4e6OZ3Q0sA9KBx919g5k9CBS6ewHwc+CXZlYEHCAaHgT9ngM2Ao3AV9y9Kd6aRDpaweoyxg85i3MGnxV2KSJxizsYANx9KbC0Rdu3YqZrgb9oY93vAd9LRB0iYSg9dIzCnQe59+pzwy5FJCE6zclnkWT18toyQOM6S9ehYBCJU8GaMqaM7MPo/j3CLkUkIRQMInEorjjC+tIqrtfegnQhCgaROLy0dg9mcN35uhpJug4Fg8gZcncK1pQxM7cfQ3rnhF2OSMIoGETO0Oa91RSVH9G9C9LlKBhEzlDBmjLS04xrJg8JuxSRhFIwiJwBd+e3a8r4xNgB9O+ZHXY5IgmlYBA5A6t3H6Lk4DEdRpIuScEgcgYK1pSRlZ7GVZMGh12KSMIpGEROU1PEeXntHi49dyC9cjLDLkck4RQMIqfp3e0HKK+u07jO0mUpGEROU8GaMrpnpXP5+EFhlyLSLhQMIqehtqGJl9eWcfWkIXTPSsjDiUWSjoJB5DS8taWcqtpGbpimcZ2l61IwiJyGX79XysCzsrn47P5hlyLSbuIKBjPrZ2avmtnW4L1vK32mmtk7ZrbBzNaa2Wdjlj1pZtvNbHXwmhpPPSLt6eDRet7cUs7cKcPISNffVNJ1xfvdfR/wuruPA14P5luqAT7v7pOA2cC/m1mfmOX3uvvU4LU6znpE2s1L6/bQ0OTcOF2HkaRrizcY5gILg+mFwA0tO7j7B+6+NZguA8qBgXF+XZEO95v3SzlncE8mDu0Vdiki7SreYBjs7nuC6b3ACW8DNbOZQBawLab5e8EhpkfMTA+dkaS0s/Ioq3Ye5MZpIzCzsMsRaVcnvd7OzF4DWnt85AOxM+7uZuYn+JyhwC+Bee4eCZrvJxooWcAC4BvAg22sPx+YDzBq1KiTlS2SUC++X4oZ3DBNN7VJ13fSYHD3WW0tM7N9ZjbU3fcEv/jL2+jXC3gZeMDdl8d89vG9jTozewL4uxPUsYBoeJCfn99mAIkkmrvz4vulXDSmP0N7dwu7HJF2F++hpAJgXjA9D1jSsoOZZQEvAr9w98Utlg0N3o3o+Yn1cdYjknDv7z7EzsoabtS9C5Ii4g2Gh4ArzWwrMCuYx8zyzeyxoM8twCXAHa1clvq0ma0D1gEDgH+Ksx6RhHvxvVKyM9KYrQF5JEXEdU+/u1cCV7TSXgjcFUw/BTzVxvqXx/P1RdpbbUMTv1ldyuzJQzhLT1KVFKG7dERO4L/X76W6tpHPfmxk2KWIdBgFg8gJLFq5i9H9u3Nhnh6BIalDwSDShh37j7K8+AC35I8kLU33LkjqUDCItOG5wt2kGdw8Y0TYpYh0KAWDSCsamyIsXlXCZecOYnCvnLDLEelQCgaRVry1pYLy6jqddJaUpGAQacWzhbsZ0DObyzR8p6QgBYNIC+VVtbyxuZybZ4wgU+MuSArSd71ICy+8V0pTxLklXyedJTUpGERiNEWcZ97dxcy8fowZ2DPsckRCoWAQifHm5nJ2Hahh3kW5YZciEhoFg0iMhe/sYEivHK6adMIxp0S6NAWDSKCovJq3t+7nry4arZPOktL03S8SWPinnWRlpHGr7l2QFKdgEAGqaht44b0S5kwZRv+eGnpcUpuCQQR4vrCEmvom7vh4btiliIQurmAws35m9qqZbQ3e+7bRrylm9LaCmPY8M1thZkVm9mwwDKhIh4pEnF+8s4MZo/syeXjvsMsRCV28ewz3Aa+7+zjg9WC+NcfcfWrwmhPT/jDwiLuPBQ4Cd8ZZj8hpe+uDcnZW1mhvQSQQbzDMBRYG0wuBG051RTMz4HJg8ZmsL5IoT/xxB4N7ZWtMZ5FAvMEw2N33BNN7gbYu/s4xs0IzW25mx3/59wcOuXtjMF8CDI+zHpHTsr70MG9v3c/nL8rVJaoigYyTdTCz14DW/pR6IHbG3d3MvI2PGe3upWY2BnjDzNYBh0+nUDObD8wHGDVq1OmsKtKmn/6hmJ7ZGXzuwtFhlyKSNE4aDO4+q61lZrbPzIa6+x4zGwqUt/EZpcF7sZm9BUwDXgD6mFlGsNcwAig9QR0LgAUA+fn5bQWQyCnbVVnDy2vL+NInx9C7W2bY5YgkjXj3nQuAecH0PGBJyw5m1tfMsoPpAcDFwEZ3d+BN4OYTrS/SXn72djEZaWl88RN5YZciklTiDYaHgCvNbCswK5jHzPLN7LGgzwSg0MzWEA2Ch9x9Y7DsG8DXzayI6DmHn8dZj8gp2VdVy7OFu/nM9OEaulOkhZMeSjoRd68ErmilvRC4K5j+E3BeG+sXAzPjqUHkTPz4rW00RZwvXzo27FJEko4uw5CUU15VyzPv7uKm6cMZ1b972OWIJB0Fg6Scn/y+mMaIc/dl48IuRSQpKRgkpZQdOsZTK3bymWnaWxBpi4JBUsp/vLYVHL42S3sLIm1RMEjKKCo/wvOrdvO5C0czoq/2FkTaomCQlPGvy7bQPSuDr1x2dtiliCQ1BYOkhBXFlfz3hr3Mv2SMBuIROQkFg3R5TRHnwZc2Mqx3Dl/65JiwyxFJegoG6fIWr9rNhrIq7rt2At2y0sMuRyTpKRikSztc08APlm1h+qg+XH/+0LDLEekU4nokhkiye3jZZg4crefJL8wkOjaUiJyM9hiky1q18yC/WrGLL1ycp7GcRU6DgkG6pPrGCA+8uI6hvXP42yvPCbsckU5Fh5KkS/rhG1vZvLean30+n57Z+jYXOR3aY5AuZ13JYR59axufmT6cKye2NQy5iLRFwSBdSm1DE/c8v5oBPbP49nWTwi5HpFOKKxjMrJ+ZvWpmW4P3vq30uczMVse8as3shmDZk2a2PWbZ1HjqEfnuSxv5YN8R/uXmKfTurnGcRc5EvHsM9wGvu/s44PVgvhl3f9Pdp7r7VOByoAb4XUyXe48vd/fVcdYjKeyVdXt4esUu/tclY/jUOQPDLkek04o3GOYCC4PphcANJ+l/M/CKu9fE+XVFmtm+/yh//8Japozswz1XnRt2OSKdWrzBMNjd9wTTe4GTnem7FXimRdv3zGytmT1iZm0+3czM5ptZoZkVVlRUxFGydDVH6hqZ/4tCMtKMH942jawMnToTicdJf4LM7DUzW9/Ka25sP3d3wE/wOUOB84BlMc33A+OBjwH9gG+0tb67L3D3fHfPHzhQhwkkKhJx7nluNdsqjvDDv5zOyH4aZ0EkXie9wNvdZ7W1zMz2mdlQd98T/OIvP8FH3QK86O4NMZ99fG+jzsyeAP7uFOsWAeCfX9nEsg37+OanJ3Dx2AFhlyPSJcS7z10AzAum5wFLTtD3NlocRgrCBIs+xOYGYH2c9UgKefKP2/nZ29v5/EWjufMTeWGXI9JlxBsMDwFXmtlWYFYwj5nlm9ljxzuZWS4wEvh9i/WfNrN1wDpgAPBPcdYjKeLF90v4zksbuXLiYL59/SQ9IE8kgeJ6VoC7VwJXtNJeCNwVM78DGN5Kv8vj+fqSml5eu4d7nlvDhXn9+a/bppGeplAQSSRdviGdypLVpXxt0fvMGN2Xn9+RT06mBt4RSTQFg3QaT6/Yyd88u5oZo/vy+B0fo3uWHo4n0h70kyVJLxJx/t+rW3j0zW1cMX4Qj94+XXsKIu1IwSBJ7WhdI/cuXsPSdXu5beYoHpw7icx07eiKtCcFgyStovJq/vqp9yiuOMID107grk/m6eojkQ6gYJCk4+4sWrmb7760kW6Z6fzyzgt085pIB1IwSFIpO3SMb/5mPW9sLufjZ/fn326ZypDeOWGXJZJSFAySFBqaIiz80w4eefUDGiPOt66byB0fzyVN9yiIdDgFg4TK3Xl9Uznff2UTxRVHufTcgXx37mQ9DE8kRAoGCUUk4izbsJcfvbWNdaWHGTOwBz/7fD6zJgzSCWaRkCkYpEPVNjTx2zVl/OT329hWcZTc/t156DPncdOMEboMVSRJKBik3bk760ureK5wN0tWl1JV28iEob34r9umce15Q/WsI5Eko2CQduHubNpTzWub9rF03R42760mKyON2ZOGcEv+SC4e21+HjESSlIJBEubA0XpW7jjAH4v289rGfZQdrsUMpo7swz/dMJnrpwyjd7fMsMsUkZNQMMgZqW+MsLW8mg1lVazZfYh3tx9ga/kRAHIy0/jkuIF8bdY4Lhs/iEFn6T4Ekc4krmAws78A/hGYAMwMxmFord9s4D+AdOAxdz8+oE8esAjoD6wC/srd6+OpSRLrcE0D2yuPsrPyKNv3H2VnZQ1by6v5YO8R6psiAPTMzmDG6L7cMG04F+T147wRvcnO0EPuRDqrePcY1gOfAX7aVgczSwceBa4ESoCVZlbg7huBh4FH3H2Rmf0EuBP4cZw1yUk0NkWoqm3k8LEGDh9r4MDROvZV1bGvqpZ9VXWUV9Wyr7qW0oPHOFjz4RDdmMGw3t0YM7AHX/hELpOG9WbSsF7k9u+hE8giXUi8I7htAk52EnEmUOTuxUHfRcBcM9sEXA78ZdBvIdG9j5QIBnenMeI0RZyGpgiNTdH5xkh0uqEpEiyLtjU0Rfs2NkVoCN7rGiMcq2/iWEMTtQ1NH07X1AfzQVt1TAhUHWuguq6x1ZrMoH+PLAadlcPgXtmcP6IPef17MLp/d/IG9GBkv+563LVICuiIcwzDgd0x8yXABUQPHx1y98aY9o8M/5lI//DiOlYUV+IOTvSXc/QdHCcSPTLSrD0SMw3+kTZvaxon4kAwHfs1I95+/8bMdCMnM53uWel0y0wnJzOdntkZDOuTw/ihZ9G7W+aHrz7do+99u2cxpHcOA3pm614CETl5MJjZa8CQVhY94O5LEl9Sm3XMB+YDjBo16ow+Y3ifbowf2guLfl7wDgakBROGNWszi/YhaE9r0ceO92nW1vrnHW+PfpyRmWakpxuZaWlkpBsZ6WlkpBkZaUZmetCWZmQEyzPT00hPMzLTo205mcEv/6y0D0NAv9hFJF4nDQZ3nxXn1ygFRsbMjwjaKoE+ZpYR7DUcb2+rjgXAAoD8/Pwz+pv7K5eNPZPVRERSSkf8ebkSGGdmeWaWBdwKFLi7A28CNwf95gEdtgciIiKtiysYzOxGMysBLgJeNrNlQfswM1sKEOwN3A0sAzYBz7n7huAjvgF83cyKiJ5z+Hk89YiISPws+od755Kfn++Fha3eMiEiIm0ws1Xunn+yfjpTKSIizSgYRESkGQWDiIg0o2AY+XPMAAAE+0lEQVQQEZFmFAwiItJMp7wqycwqgJ1nuPoAYH8Cy2kPyV5jstcHyV9jstcHqjERkq2+0e4+8GSdOmUwxMPMCk/lcq0wJXuNyV4fJH+NyV4fqMZESPb62qJDSSIi0oyCQUREmknFYFgQdgGnINlrTPb6IPlrTPb6QDUmQrLX16qUO8cgIiInlop7DCIicgIpFQxmNtvMtphZkZndlwT1jDSzN81so5ltMLOvBe39zOxVM9savPcNuc50M3vfzF4K5vPMbEWwHZ8NHqceZn19zGyxmW02s01mdlESbsO/Df6P15vZM2aWE/Z2NLPHzazczNbHtLW63SzqP4Na15rZ9JDq+0Hw/7zWzF40sz4xy+4P6ttiZle3d31t1Riz7B4zczMbEMx3+DY8UykTDGaWDjwKXANMBG4zs4nhVkUjcI+7TwQuBL4S1HQf8Lq7jwNeD+bD9DWij0w/7mHgEXcfCxwE7gylqj/7D+C/3X08MIVorUmzDc1sOPBVIN/dJwPpRMclCXs7PgnMbtHW1na7BhgXvObTMWOzt1bfq8Bkdz8f+AC4HyD4ubkVmBSs86PgZz6MGjGzkcBVwK6Y5jC24RlJmWAAZgJF7l7s7vXAImBumAW5+x53fy+Yrib6C214UNfCoNtC4IZwKgQzGwF8GngsmDfgcmBx0CXs+noDlxCM5eHu9e5+iCTahoEMoJuZZQDdgT2EvB3d/Q/AgRbNbW23ucAvPGo50dEXh3Z0fe7+u5hx4pcTHfnxeH2L3L3O3bcDRUR/5ttVG9sQ4BHg74kO9X5ch2/DM5VKwTAc2B0zXxK0JQUzywWmASuAwe6+J1i0FxgcUlkA/070GzwSzPcHDsX8cIa9HfOACuCJ4HDXY2bWgyTahu5eCvwr0b8e9wCHgVUk13Y8rq3tlow/P18EXgmmk6Y+M5sLlLr7mhaLkqbGk0mlYEhaZtYTeAH4G3evil0WDIEayqVjZnYdUO7uq8L4+qcoA5gO/NjdpwFHaXHYKMxtCBAcp59LNMSGAT1o5fBDsgl7u52ImT1A9FDs02HXEsvMugP/AHwr7FrikUrBUAqMjJkfEbSFyswyiYbC0+7+66B53/FdzOC9PKTyLgbmmNkOoofeLid6PL9PcEgEwt+OJUCJu68I5hcTDYpk2YYAs4Dt7l7h7g3Ar4lu22Tajse1td2S5ufHzO4ArgNu9z9fb58s9Z1N9A+ANcHPzQjgPTMbQvLUeFKpFAwrgXHBlSBZRE9UFYRZUHC8/ufAJnf/t5hFBcC8YHoesKSjawNw9/vdfYS75xLdXm+4++3Am8DNYdcH4O57gd1mdm7QdAWwkSTZhoFdwIVm1j34Pz9eY9JsxxhtbbcC4PPBlTUXAodjDjl1GDObTfTQ5hx3r4lZVADcambZZpZH9ATvux1dn7uvc/dB7p4b/NyUANOD79Ok2IanxN1T5gVcS/RKhm3AA0lQzyeI7qqvBVYHr2uJHsd/HdgKvAb0S4JaLwVeCqbHEP2hKwKeB7JDrm0qUBhsx98AfZNtGwLfATYD64FfAtlhb0fgGaLnPBqI/gK7s63tBhjRq/q2AeuIXmEVRn1FRI/TH/95+UlM/weC+rYA14S1DVss3wEMCGsbnulLdz6LiEgzqXQoSUREToGCQUREmlEwiIhIMwoGERFpRsEgIiLNKBhERKQZBYOIiDSjYBARkWb+Pw3DbPknLKwCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "series2  = read_csv('data/data-test-tanh-nonoise.csv', header=0, index_col=0, squeeze=True)\n",
    "\n",
    "n_test2 = 100\n",
    "scaler2, train2, test2 = prepare_data(series2, n_test2, n_lag, n_seq)\n",
    "\n",
    "forecasts2 = make_forecasts(model, n_batch, train2, test2, n_lag, n_seq)\n",
    "\n",
    "# inverse transform forecasts and test\n",
    "forecasts2 = inverse_transform(series2, forecasts2, scaler2, n_test2+2)\n",
    "actual2 = [row[n_lag:] for row in test2]\n",
    "actual2 = inverse_transform(series2, actual2, scaler2, n_test2+2)\n",
    "# evaluate forecasts\n",
    "#evaluate_forecasts(actual2, forecasts2, n_lag, n_seq)\n",
    "# plot forecasts\n",
    "plot_forecasts2(series2, forecasts2, n_test2+2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
